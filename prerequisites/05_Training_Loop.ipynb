{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 5: The Standard Training Loop ðŸ”„\n",
                "\n",
                "We've written the same 5 lines of training code twice now. It's time to organize it.\n",
                "\n",
                "In this notebook, we'll build a **robust, reusable training loop**."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from sklearn.model_selection import train_test_split\n",
                "import matplotlib.pyplot as plt\n",
                "%matplotlib inline"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Setup Data (Train/Val Split)\n",
                "\n",
                "Real ML requires splitting data into **Training** (to learn) and **Validation** (to test).\n",
                "\n",
                "Let's use our quadratic data again."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Generate data\n",
                "X = torch.linspace(-5, 5, 200).reshape(-1, 1)\n",
                "y = X ** 2 + 1 + torch.randn(200, 1) * 2\n",
                "\n",
                "# Split (80% Train, 20% Validation)\n",
                "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=42)\n",
                "\n",
                "# Convert back to tensors (sklearn returns numpy arrays)\n",
                "X_train = torch.tensor(X_train, dtype=torch.float32)\n",
                "X_val = torch.tensor(X_val, dtype=torch.float32)\n",
                "y_train = torch.tensor(y_train, dtype=torch.float32)\n",
                "y_val = torch.tensor(y_val, dtype=torch.float32)\n",
                "\n",
                "print(f\"Train Shape: {X_train.shape}, Val Shape: {X_val.shape}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Define Function for One Training Step\n",
                "\n",
                "Let's wrap the \"Forward -> Loss -> Backward -> Step\" logic into a function."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def train_step(model, X_batch, y_batch, loss_fn, optimizer):\n",
                "    # 1. Set model to training mode (important for Dropout/BatchNorm)\n",
                "    model.train()\n",
                "    \n",
                "    # 2. Forward pass\n",
                "    predictions = model(X_batch)\n",
                "    \n",
                "    # 3. Compute loss\n",
                "    loss = loss_fn(predictions, y_batch)\n",
                "    \n",
                "    # 4. Backward pass\n",
                "    optimizer.zero_grad()\n",
                "    loss.backward()\n",
                "    \n",
                "    # 5. Optimizer step\n",
                "    optimizer.step()\n",
                "    \n",
                "    return loss.item()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Define Function for Validation\n",
                "\n",
                "We also need a function to check performance without training (inference)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "def val_step(model, X_batch, y_batch, loss_fn):\n",
                "    # 1. Set model to eval mode\n",
                "    model.eval()\n",
                "    \n",
                "    # 2. Disable gradient calculation\n",
                "    with torch.no_grad():\n",
                "        predictions = model(X_batch)\n",
                "        loss = loss_fn(predictions, y_batch)\n",
                "    \n",
                "    return loss.item()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. The Loop\n",
                "\n",
                "Now we just loop over epochs and call our functions."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Model, Loss, Optimizer\n",
                "model = nn.Sequential(\n",
                "    nn.Linear(1, 20),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(20, 20),\n",
                "    nn.ReLU(),\n",
                "    nn.Linear(20, 1)\n",
                ")\n",
                "\n",
                "loss_fn = nn.MSELoss()\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
                "\n",
                "# Training\n",
                "epochs = 500\n",
                "train_losses = []\n",
                "val_losses = []\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    train_loss = train_step(model, X_train, y_train, loss_fn, optimizer)\n",
                "    val_loss = val_step(model, X_val, y_val, loss_fn)\n",
                "    \n",
                "    train_losses.append(train_loss)\n",
                "    val_losses.append(val_loss)\n",
                "    \n",
                "    if (epoch+1) % 50 == 0:\n",
                "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 5. Plot Loss Curves\n",
                "\n",
                "Crucial for debugging!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "plt.plot(train_losses, label='Train Loss')\n",
                "plt.plot(val_losses, label='Val Loss')\n",
                "plt.legend()\n",
                "plt.title(\"Training vs Validation Loss\")\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Summary\n",
                "\n",
                "1. **`model.train()`** vs **`model.eval()`**: Switches behavior (e.g. Dropout).\n",
                "2. **`optimizer.zero_grad()`** MUST be called before `.step()`.\n",
                "3. **`with torch.no_grad()`**: Use this for validation/inference.\n",
                "4. Always monitor **Train vs Val loss** to spot overfitting.\n",
                "\n",
                "Next up: **DataLoaders** - Handling messy real-world data!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}