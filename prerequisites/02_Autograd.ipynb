{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 2: Autograd - The Engine of Learning ðŸš‚\n",
                "\n",
                "In Part 1, we saw that Tensors are just n-dimensional arrays.\n",
                "\n",
                "In this notebook, we unlock their true power: **Automatic Differentiation (Autograd)**.\n",
                "\n",
                "**Why do we need this?**\n",
                "To train a neural network, we need to know how to adjust its weights to minimize error. This requires calculating gradients (calculus). PyTorch does this calculus for us automatically!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Tracking History\n",
                "\n",
                "By default, tensors don't track their history. We have to tell them to."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# A standard tensor\n",
                "x = torch.tensor(2.0)\n",
                "\n",
                "# A tensor we want to optimize (like a weight in a neural net)\n",
                "w = torch.tensor(3.0, requires_grad=True)\n",
                "\n",
                "print(f\"x requires_grad: {x.requires_grad}\")\n",
                "print(f\"w requires_grad: {w.requires_grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Computational Graph\n",
                "\n",
                "Any operation we do on `w` will be recorded in a generic graph."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's do some math: y = x * w + 2\n",
                "# y = 2 * 3 + 2 = 8\n",
                "\n",
                "y = x * w + 2\n",
                "\n",
                "print(\"Result y:\", y)\n",
                "print(\"Function that created y:\", y.grad_fn)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "See that `AddBackward0`? PyTorch knows `y` came from an addition."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Let's go deeper: z = y^2\n",
                "# z = 8^2 = 64\n",
                "\n",
                "z = y ** 2\n",
                "print(\"Result z:\", z)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Backpropagation (The Magic)\n",
                "\n",
                "Now we want to know: **How much does `z` change if we change `w`?**\n",
                "\n",
                "Mathematically, we want $\\frac{dz}{dw}$.\n",
                "\n",
                "We could do this by hand (chain rule):\n",
                "1. $z = y^2 \\rightarrow \\frac{dz}{dy} = 2y = 16$\n",
                "2. $y = xw + 2 \\rightarrow \\frac{dy}{dw} = x = 2$\n",
                "3. $\\frac{dz}{dw} = \\frac{dz}{dy} \\cdot \\frac{dy}{dw} = 16 \\cdot 2 = 32$\n",
                "\n",
                "Or... we can just call `.backward()`!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Compute gradients\n",
                "z.backward()\n",
                "\n",
                "# Check the gradient of w\n",
                "print(f\"The gradient dz/dw is: {w.grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "Boom! It matches our manual calculation (32). \n",
                "\n",
                "**Note:** `x.grad` will be None because we didn't set `requires_grad=True` for it."
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 4. Stopping Autograd\n",
                "\n",
                "When we are just using the model to make predictions (Inference), we don't need to track gradients. It wastes memory. \n",
                "\n",
                "We use `torch.no_grad()` to temporarily disable it."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "print(f\"Before context: {w.requires_grad}\")\n",
                "\n",
                "with torch.no_grad():\n",
                "    y_new = w * 5\n",
                "    print(f\"Inside context: {y_new.requires_grad}\")\n",
                "    \n",
                "print(f\"After context: {w.requires_grad}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Summary\n",
                "\n",
                "1. **`requires_grad=True`** enables gradient tracking.\n",
                "2. **`.backward()`** computes gradients automatically.\n",
                "3. **`.grad`** stores the computed gradient.\n",
                "4. **`torch.no_grad()`** disables tracking for efficiency.\n",
                "\n",
                "Next up: **Linear Regression** - Building our first actual model!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}