{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 6: Datasets & DataLoaders ðŸ“¦\n",
                "\n",
                "In the previous notebooks, we manually sliced our tensors (`X_train`, `y_train`). \n",
                "For huge datasets (like 1TB of images), you can't load everything into memory at once.\n",
                "\n",
                "PyTorch solves this with **Datasets** (how to get one item) and **DataLoaders** (how to batch them)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import torch.nn as nn\n",
                "import torch.optim as optim\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "import numpy as np"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Custom Dataset Class\n",
                "\n",
                "To use PyTorch's data tools, we just need to create a class that inherits from `Dataset` and implements:\n",
                "1. `__len__`: How many items are there?\n",
                "2. `__getitem__`: Get the i-th item."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "class MyCustomDataset(Dataset):\n",
                "    def __init__(self, size=1000):\n",
                "        # Generate fake data on init\n",
                "        self.x = torch.rand(size, 1) * 10\n",
                "        self.y = self.x ** 2 + 1 + torch.randn(size, 1) * 2\n",
                "        \n",
                "    def __len__(self):\n",
                "        return len(self.x)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        return self.x[idx], self.y[idx]\n",
                "\n",
                "# Instantiate\n",
                "dataset = MyCustomDataset(size=500)\n",
                "print(f\"Dataset size: {len(dataset)}\")\n",
                "print(f\"Item 0: {dataset[0]}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Using DataLoader\n",
                "\n",
                "`DataLoader` takes a Dataset and gives you an iterator that handles:\n",
                "- Batching (e.g., 32 items at a time)\n",
                "- Shuffling (random order)\n",
                "- Parallel loading (`num_workers`)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
                "\n",
                "# Iterate through one batch\n",
                "for X_batch, y_batch in dataloader:\n",
                "    print(f\"Batch Shape X: {X_batch.shape}\")\n",
                "    print(f\"Batch Shape y: {y_batch.shape}\")\n",
                "    break"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. Training Loop with DataLoader\n",
                "\n",
                "This is how real training loops look."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# Setup Model\n",
                "model = nn.Sequential(\n",
                "    nn.Linear(1, 20), \n",
                "    nn.ReLU(), \n",
                "    nn.Linear(20, 1)\n",
                ")\n",
                "optimizer = optim.Adam(model.parameters(), lr=0.01)\n",
                "criterion = nn.MSELoss()\n",
                "\n",
                "# Loop\n",
                "epochs = 5\n",
                "\n",
                "for epoch in range(epochs):\n",
                "    total_loss = 0\n",
                "    \n",
                "    for X_batch, y_batch in dataloader:\n",
                "        # The standard 5 steps\n",
                "        predictions = model(X_batch)\n",
                "        loss = criterion(predictions, y_batch)\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        optimizer.step()\n",
                "        \n",
                "        total_loss += loss.item()\n",
                "    \n",
                "    avg_loss = total_loss / len(dataloader)\n",
                "    print(f\"Epoch {epoch+1}: Avg Loss = {avg_loss:.4f}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Summary\n",
                "\n",
                "1. **`Dataset`**: Defines HOW to get a single item.\n",
                "2. **`DataLoader`**: Defines HOW to batch and shuffle items.\n",
                "3. **Training Loop**: Iterates over the `DataLoader` instead of raw tensors.\n",
                "\n",
                "Next up: **CNNs** - Finally, we work with images!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}