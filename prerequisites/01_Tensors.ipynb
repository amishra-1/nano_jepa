{
    "cells": [
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "# Part 1: The Magic of Tensors ðŸª„\n",
                "\n",
                "Welcome to your first step in PyTorch! \n",
                "\n",
                "In deep learning, data is represented as **Tensors**. \n",
                "\n",
                "Think of a Tensor as a **Super-Power Array**. It looks and behaves like a standard list or table of numbers (like in NumPy or Excel), but it has two special abilities:\n",
                "1. **GPU Acceleration**: It can do math incredibly fast on a graphics card.\n",
                "2. **Automatic Differentiation**: It remembers the math operations performed on it (we'll see this in Part 2).\n",
                "\n",
                "Let's dive in!"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "import torch\n",
                "import numpy as np\n",
                "\n",
                "print(f\"PyTorch version: {torch.__version__}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 1. Creating Tensors\n",
                "\n",
                "We can create tensors from standard Python lists or NumPy arrays."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "# From a list\n",
                "scalar = torch.tensor(7)\n",
                "vector = torch.tensor([1, 2, 3])\n",
                "matrix = torch.tensor([[1, 2], [3, 4]])\n",
                "\n",
                "print(f\"Scalar: {scalar.item()} (Shape: {scalar.shape})\")\n",
                "print(f\"Vector: {vector} (Shape: {vector.shape})\")\n",
                "print(f\"Matrix:\\n{matrix} (Shape: {matrix.shape})\")"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Helper Functions\n",
                "Just like NumPy, PyTorch has helpers to create common tensors."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "zeros = torch.zeros(2, 3)       # Matrix of zeros\n",
                "ones = torch.ones(2, 3)         # Matrix of ones\n",
                "rand = torch.rand(2, 3)         # Random numbers between 0 and 1\n",
                "randn = torch.randn(2, 3)       # Random numbers from normal distribution (bell curve)\n",
                "\n",
                "print(\"Random Tensor:\\n\", randn)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 2. Math & Operations\n",
                "\n",
                "Tensors feel exactly like NumPy arrays. You can add, multiply, and manipulate them."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "a = torch.tensor([1, 2, 3])\n",
                "b = torch.tensor([4, 5, 6])\n",
                "\n",
                "# Element-wise operations\n",
                "print(\"Addition:\", a + b)\n",
                "print(\"Multiplication:\", a * b)\n",
                "\n",
                "# Matrix Multiplication (Dot Product)\n",
                "# 1*4 + 2*5 + 3*6 = 4 + 10 + 18 = 32\n",
                "print(\"Dot Product:\", torch.matmul(a, b))  # or a @ b"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "### Reshaping/View\n",
                "Changing the shape of a tensor is very common in Deep Learning (e.g., flattening an image)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "x = torch.rand(16)\n",
                "print(\"Original:\", x.shape)\n",
                "\n",
                "reshaped = x.view(4, 4)  # Reshape to 4x4 matrix\n",
                "print(\"Reshaped:\", reshaped.shape)\n",
                "\n",
                "flattened = reshaped.view(-1) # -1 means \"infer this dimension\"\n",
                "print(\"Flattened:\", flattened.shape)"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## 3. The Power of Hardware (MPS/GPU)\n",
                "\n",
                "This is why we use PyTorch. We can move tensors to the GPU for faster visualization.\n",
                "\n",
                "On Mac (Apple Silicon), we use `'mps'` (Metal Performance Shaders).\n",
                "On Windows/Linux with NVIDIA, we use `'cuda'`."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "metadata": {},
            "outputs": [],
            "source": [
                "if torch.backends.mps.is_available():\n",
                "    device = torch.device(\"mps\")\n",
                "    print(\"ðŸš€ MPS (Apple Metal) is available!\")\n",
                "elif torch.cuda.is_available():\n",
                "    device = torch.device(\"cuda\")\n",
                "    print(\"ðŸš€ CUDA (NVIDIA) is available!\")\n",
                "else:\n",
                "    device = torch.device(\"cpu\")\n",
                "    print(\"ðŸ’» Using CPU (No accelerator found)\")\n",
                "\n",
                "# Move tensor to device\n",
                "x = torch.tensor([1, 2, 3])\n",
                "x_gpu = x.to(device)\n",
                "\n",
                "print(f\"Tensor is on: {x_gpu.device}\")\n",
                "\n",
                "# Note: You can't mix devices! \n",
                "# x + x_gpu  # This would throw an error"
            ]
        },
        {
            "cell_type": "markdown",
            "metadata": {},
            "source": [
                "## ðŸ§  Summary\n",
                "\n",
                "1. **Tensors** are n-dimensional arrays.\n",
                "2. They behave like **NumPy** arrays (slicing, math, etc.).\n",
                "3. They can exist on the **GPU/MPS** for massive speedups.\n",
                "\n",
                "Next up: **Autograd** - How PyTorch learns calculus for you!"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.8.5"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 4
}